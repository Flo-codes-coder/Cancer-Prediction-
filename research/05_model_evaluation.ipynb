{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Cancer-Prediction-\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Cancer-Prediction-'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/kevoa/Cancer-Prediction-.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"kevoa\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"05ede0b26068798db0131aa3a65a1251ceaa1a4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n",
    "    mlflow_username: str\n",
    "    mlflow_password: str\n",
    "    important_features: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CancerPrediction.constants import *\n",
    "from CancerPrediction.utils.common import read_yaml, create_directories\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([Path(self.config['artifacts_root'])])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config['model_evaluation']\n",
    "        \n",
    "        create_directories([Path(config['root_dir'])])\n",
    "        \n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(config['root_dir']),\n",
    "            test_data_path=Path(config['test_data_path']),\n",
    "            model_path=Path(config['model_path']),\n",
    "            metric_file_name=Path(config['metric_file_name']),\n",
    "            mlflow_uri=config['mlflow_uri'],\n",
    "            mlflow_username=config['mlflow_username'],\n",
    "            mlflow_password=config['mlflow_password'],\n",
    "            target_column=config['target_column'],\n",
    "            important_features=config['important_features']  # Añadir esta línea\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 13:29:18,835: INFO: common: YAML file: config\\config.yaml loaded successfully]\n",
      "[2024-06-27 13:29:18,840: INFO: common: YAML file: params.yaml loaded successfully]\n",
      "[2024-06-27 13:29:18,844: INFO: common: YAML file: schema.yaml loaded successfully]\n",
      "[2024-06-27 13:29:18,847: INFO: common: Created directory at: artifacts]\n",
      "[2024-06-27 13:29:18,849: INFO: common: Created directory at: artifacts\\model_evaluation]\n",
      "[2024-06-27 13:29:20,022: INFO: common: JSON file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'VotingClassifierModel' already exists. Creating a new version of this model...\n",
      "2024/06/27 13:29:34 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: VotingClassifierModel, version 6\n",
      "Created version '6' of model 'VotingClassifierModel'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "from CancerPrediction import logger\n",
    "from CancerPrediction.utils.common import save_json\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def eval_metrics(self, y_true, y_pred):\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        test_data = pd.read_excel(self.config.test_data_path)\n",
    "        model = joblib.load(self.config.model_path)\n",
    "\n",
    "        # Seleccionar características importantes directamente de la configuración\n",
    "        X_test = test_data[self.config.important_features]\n",
    "        y_test = test_data[self.config.target_column]\n",
    "\n",
    "        # Configurar MLflow\n",
    "        os.environ[\"MLFLOW_TRACKING_URI\"] = self.config.mlflow_uri\n",
    "        os.environ[\"MLFLOW_TRACKING_USERNAME\"] = self.config.mlflow_username\n",
    "        os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = self.config.mlflow_password\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Realizar predicciones y evaluar el modelo en el conjunto de prueba\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Calcular métricas\n",
    "            precision, recall, f1 = self.eval_metrics(y_test, y_pred)\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "            # Guardar métricas localmente\n",
    "            scores = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1_score\": f1,\n",
    "                \"confusion_matrix\": conf_matrix.tolist(),\n",
    "                \"classification_report\": class_report\n",
    "            }\n",
    "            save_json(path=self.config.metric_file_name, data=scores)\n",
    "\n",
    "            # Registrar métricas en MLflow\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.log_artifact(self.config.metric_file_name)\n",
    "\n",
    "            # Registrar el modelo\n",
    "            mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"VotingClassifierModel\")\n",
    "\n",
    "try:\n",
    "    config_manager = ConfigurationManager()\n",
    "    model_evaluation_config = config_manager.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel(r\"C:\\Cancer-Prediction-\\artifacts\\data_transformation\\test_df.xlsx\")\n",
    "train_df = pd.read_excel(r\"C:\\Cancer-Prediction-\\artifacts\\data_transformation\\train_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFP (pg/ml)</th>\n",
       "      <th>Angiopoietin-2 (pg/ml)</th>\n",
       "      <th>AXL (pg/ml)</th>\n",
       "      <th>CA-125 (U/ml)</th>\n",
       "      <th>CA 15-3 (U/ml)</th>\n",
       "      <th>CA19-9 (U/ml)</th>\n",
       "      <th>CD44 (ng/ml)</th>\n",
       "      <th>CEA (pg/ml)</th>\n",
       "      <th>CYFRA 21-1 (pg/ml)</th>\n",
       "      <th>DKK1 (ng/ml)</th>\n",
       "      <th>...</th>\n",
       "      <th>sHER2/sEGFR2/sErbB2 (pg/ml)</th>\n",
       "      <th>sPECAM-1 (pg/ml)</th>\n",
       "      <th>TGFa (pg/ml)</th>\n",
       "      <th>Thrombospondin-2 (pg/ml)</th>\n",
       "      <th>TIMP-1 (pg/ml)</th>\n",
       "      <th>TIMP-2 (pg/ml)</th>\n",
       "      <th>Omega score</th>\n",
       "      <th>AJCC Stage</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>Tumor type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.162730</td>\n",
       "      <td>-0.399967</td>\n",
       "      <td>0.161177</td>\n",
       "      <td>-0.155628</td>\n",
       "      <td>-0.208773</td>\n",
       "      <td>-0.126586</td>\n",
       "      <td>0.100087</td>\n",
       "      <td>-0.177265</td>\n",
       "      <td>-0.118556</td>\n",
       "      <td>-0.748740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613042</td>\n",
       "      <td>-0.739088</td>\n",
       "      <td>-0.190133</td>\n",
       "      <td>-0.501581</td>\n",
       "      <td>-0.709657</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>-0.168524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.161972</td>\n",
       "      <td>-0.412345</td>\n",
       "      <td>-0.862979</td>\n",
       "      <td>-0.155856</td>\n",
       "      <td>-0.139035</td>\n",
       "      <td>-0.126858</td>\n",
       "      <td>-0.260427</td>\n",
       "      <td>-0.162648</td>\n",
       "      <td>-0.117902</td>\n",
       "      <td>-0.268988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502477</td>\n",
       "      <td>-0.480916</td>\n",
       "      <td>-0.191864</td>\n",
       "      <td>-0.504778</td>\n",
       "      <td>-0.669592</td>\n",
       "      <td>-0.306917</td>\n",
       "      <td>-0.239250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.155496</td>\n",
       "      <td>-0.507819</td>\n",
       "      <td>-0.284533</td>\n",
       "      <td>-0.155194</td>\n",
       "      <td>-0.255232</td>\n",
       "      <td>0.053126</td>\n",
       "      <td>-0.630079</td>\n",
       "      <td>-0.184200</td>\n",
       "      <td>-0.115457</td>\n",
       "      <td>0.867268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710243</td>\n",
       "      <td>-0.461172</td>\n",
       "      <td>-0.182823</td>\n",
       "      <td>-0.367130</td>\n",
       "      <td>-0.404068</td>\n",
       "      <td>-1.120939</td>\n",
       "      <td>-0.243808</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.150629</td>\n",
       "      <td>-0.584907</td>\n",
       "      <td>-0.928337</td>\n",
       "      <td>-0.155787</td>\n",
       "      <td>-0.230770</td>\n",
       "      <td>-0.106049</td>\n",
       "      <td>0.047755</td>\n",
       "      <td>-0.181969</td>\n",
       "      <td>-0.011969</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.584788</td>\n",
       "      <td>-0.635555</td>\n",
       "      <td>-0.198597</td>\n",
       "      <td>-0.488231</td>\n",
       "      <td>-0.154741</td>\n",
       "      <td>1.505864</td>\n",
       "      <td>-0.042227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.147157</td>\n",
       "      <td>-0.464835</td>\n",
       "      <td>1.144186</td>\n",
       "      <td>-0.131803</td>\n",
       "      <td>0.317763</td>\n",
       "      <td>-0.085735</td>\n",
       "      <td>2.054673</td>\n",
       "      <td>-0.192859</td>\n",
       "      <td>-0.119066</td>\n",
       "      <td>-1.203243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>1.377542</td>\n",
       "      <td>-0.207830</td>\n",
       "      <td>-0.195666</td>\n",
       "      <td>-0.340628</td>\n",
       "      <td>1.733127</td>\n",
       "      <td>-0.215178</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.162529</td>\n",
       "      <td>-0.168476</td>\n",
       "      <td>-0.876422</td>\n",
       "      <td>-0.155536</td>\n",
       "      <td>-0.239253</td>\n",
       "      <td>-0.126601</td>\n",
       "      <td>-0.726438</td>\n",
       "      <td>-0.186948</td>\n",
       "      <td>-0.065832</td>\n",
       "      <td>-0.092237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172675</td>\n",
       "      <td>-0.547599</td>\n",
       "      <td>-0.189364</td>\n",
       "      <td>-0.360039</td>\n",
       "      <td>-0.860562</td>\n",
       "      <td>-0.525791</td>\n",
       "      <td>-0.224797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.142995</td>\n",
       "      <td>-0.562578</td>\n",
       "      <td>-0.431811</td>\n",
       "      <td>-0.106853</td>\n",
       "      <td>-0.020372</td>\n",
       "      <td>-0.032200</td>\n",
       "      <td>0.319386</td>\n",
       "      <td>-0.146195</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>-0.672990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559587</td>\n",
       "      <td>0.196013</td>\n",
       "      <td>0.507743</td>\n",
       "      <td>-0.076363</td>\n",
       "      <td>-0.091691</td>\n",
       "      <td>2.174226</td>\n",
       "      <td>-0.267385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.148364</td>\n",
       "      <td>-0.353302</td>\n",
       "      <td>-0.605628</td>\n",
       "      <td>-0.155856</td>\n",
       "      <td>-0.237970</td>\n",
       "      <td>-0.126858</td>\n",
       "      <td>-0.399981</td>\n",
       "      <td>-0.201005</td>\n",
       "      <td>-0.117902</td>\n",
       "      <td>-1.051742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210727</td>\n",
       "      <td>-0.548275</td>\n",
       "      <td>-0.191864</td>\n",
       "      <td>-0.504778</td>\n",
       "      <td>-0.467507</td>\n",
       "      <td>0.435235</td>\n",
       "      <td>-0.242274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.161157</td>\n",
       "      <td>-0.206974</td>\n",
       "      <td>0.054145</td>\n",
       "      <td>-0.156015</td>\n",
       "      <td>-0.186974</td>\n",
       "      <td>-0.126786</td>\n",
       "      <td>-0.694872</td>\n",
       "      <td>-0.208696</td>\n",
       "      <td>-0.119163</td>\n",
       "      <td>-0.874991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232628</td>\n",
       "      <td>-0.567034</td>\n",
       "      <td>-0.204752</td>\n",
       "      <td>-0.378567</td>\n",
       "      <td>-1.113493</td>\n",
       "      <td>-0.971085</td>\n",
       "      <td>-0.240743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.161577</td>\n",
       "      <td>-0.482293</td>\n",
       "      <td>-1.306079</td>\n",
       "      <td>-0.100463</td>\n",
       "      <td>-0.246552</td>\n",
       "      <td>-0.093165</td>\n",
       "      <td>-0.637555</td>\n",
       "      <td>-0.170552</td>\n",
       "      <td>-0.091464</td>\n",
       "      <td>-1.152742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873070</td>\n",
       "      <td>-0.961481</td>\n",
       "      <td>-0.202700</td>\n",
       "      <td>-0.379586</td>\n",
       "      <td>-0.877129</td>\n",
       "      <td>-0.756096</td>\n",
       "      <td>-0.223546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AFP (pg/ml)  Angiopoietin-2 (pg/ml)  AXL (pg/ml)  CA-125 (U/ml)  \\\n",
       "0      -0.162730               -0.399967     0.161177      -0.155628   \n",
       "1      -0.161972               -0.412345    -0.862979      -0.155856   \n",
       "2      -0.155496               -0.507819    -0.284533      -0.155194   \n",
       "3      -0.150629               -0.584907    -0.928337      -0.155787   \n",
       "4      -0.147157               -0.464835     1.144186      -0.131803   \n",
       "..           ...                     ...          ...            ...   \n",
       "297    -0.162529               -0.168476    -0.876422      -0.155536   \n",
       "298    -0.142995               -0.562578    -0.431811      -0.106853   \n",
       "299    -0.148364               -0.353302    -0.605628      -0.155856   \n",
       "300    -0.161157               -0.206974     0.054145      -0.156015   \n",
       "301    -0.161577               -0.482293    -1.306079      -0.100463   \n",
       "\n",
       "     CA 15-3 (U/ml)  CA19-9 (U/ml)  CD44 (ng/ml)  CEA (pg/ml)  \\\n",
       "0         -0.208773      -0.126586      0.100087    -0.177265   \n",
       "1         -0.139035      -0.126858     -0.260427    -0.162648   \n",
       "2         -0.255232       0.053126     -0.630079    -0.184200   \n",
       "3         -0.230770      -0.106049      0.047755    -0.181969   \n",
       "4          0.317763      -0.085735      2.054673    -0.192859   \n",
       "..              ...            ...           ...          ...   \n",
       "297       -0.239253      -0.126601     -0.726438    -0.186948   \n",
       "298       -0.020372      -0.032200      0.319386    -0.146195   \n",
       "299       -0.237970      -0.126858     -0.399981    -0.201005   \n",
       "300       -0.186974      -0.126786     -0.694872    -0.208696   \n",
       "301       -0.246552      -0.093165     -0.637555    -0.170552   \n",
       "\n",
       "     CYFRA 21-1 (pg/ml)  DKK1 (ng/ml)  ...  sHER2/sEGFR2/sErbB2 (pg/ml)  \\\n",
       "0             -0.118556     -0.748740  ...                     0.613042   \n",
       "1             -0.117902     -0.268988  ...                    -0.502477   \n",
       "2             -0.115457      0.867268  ...                    -0.710243   \n",
       "3             -0.011969      0.210765  ...                    -0.584788   \n",
       "4             -0.119066     -1.203243  ...                     0.827342   \n",
       "..                  ...           ...  ...                          ...   \n",
       "297           -0.065832     -0.092237  ...                    -0.172675   \n",
       "298            0.002598     -0.672990  ...                     0.559587   \n",
       "299           -0.117902     -1.051742  ...                    -0.210727   \n",
       "300           -0.119163     -0.874991  ...                    -0.232628   \n",
       "301           -0.091464     -1.152742  ...                    -0.873070   \n",
       "\n",
       "     sPECAM-1 (pg/ml)  TGFa (pg/ml)  Thrombospondin-2 (pg/ml)  TIMP-1 (pg/ml)  \\\n",
       "0           -0.739088     -0.190133                 -0.501581       -0.709657   \n",
       "1           -0.480916     -0.191864                 -0.504778       -0.669592   \n",
       "2           -0.461172     -0.182823                 -0.367130       -0.404068   \n",
       "3           -0.635555     -0.198597                 -0.488231       -0.154741   \n",
       "4            1.377542     -0.207830                 -0.195666       -0.340628   \n",
       "..                ...           ...                       ...             ...   \n",
       "297         -0.547599     -0.189364                 -0.360039       -0.860562   \n",
       "298          0.196013      0.507743                 -0.076363       -0.091691   \n",
       "299         -0.548275     -0.191864                 -0.504778       -0.467507   \n",
       "300         -0.567034     -0.204752                 -0.378567       -1.113493   \n",
       "301         -0.961481     -0.202700                 -0.379586       -0.877129   \n",
       "\n",
       "     TIMP-2 (pg/ml)  Omega score  AJCC Stage  Sex_1  Tumor type  \n",
       "0          0.506555    -0.168524           0      1           1  \n",
       "1         -0.306917    -0.239250           1      1           4  \n",
       "2         -1.120939    -0.243808           1      1           1  \n",
       "3          1.505864    -0.042227           1      0           1  \n",
       "4          1.733127    -0.215178           0      1           6  \n",
       "..              ...          ...         ...    ...         ...  \n",
       "297       -0.525791    -0.224797           0      0           0  \n",
       "298        2.174226    -0.267385           1      0           6  \n",
       "299        0.435235    -0.242274           0      0           0  \n",
       "300       -0.971085    -0.240743           0      0           1  \n",
       "301       -0.756096    -0.223546           0      0           7  \n",
       "\n",
       "[1005 rows x 43 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 11:48:03,548: INFO: null: Guidance: There are no missing values in column sFas (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:03,619: INFO: null: Guidance: There are no missing values in column sHER2/sEGFR2/sErbB2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:03,814: INFO: null: Guidance: There are no missing values in column CA 15-3 (U/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:03,854: INFO: null: Guidance: There are no missing values in column CA19-9 (U/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:03,882: INFO: null: Guidance: There are no missing values in column CA-125 (U/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:03,918: INFO: null: Guidance: There are no missing values in column TIMP-2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,065: INFO: null: Guidance: There are no missing values in column TGFa (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,065: INFO: null: Guidance: There are no missing values in column Sex_1. Extra column not created.]\n",
      "[2024-06-27 11:48:04,085: INFO: null: Guidance: There are no missing values in column Leptin (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,134: INFO: null: Guidance: There are no missing values in column IL-8 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,165: INFO: null: Guidance: There are no missing values in column IL-6 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,253: INFO: null: Guidance: There are no missing values in column AFP (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,281: INFO: null: Guidance: There are no missing values in column GDF15 (ng/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,353: INFO: null: Guidance: There are no missing values in column Prolactin (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,436: INFO: null: Guidance: There are no missing values in column HGF (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,586: INFO: null: Guidance: There are no missing values in column CD44 (ng/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,716: INFO: null: Guidance: There are no missing values in column Midkine (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,781: INFO: null: Guidance: There are no missing values in column Thrombospondin-2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,852: INFO: null: Guidance: There are no missing values in column TIMP-1 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 11:48:04,919: INFO: null: Guidance: There are no missing values in column HE4 (pg/ml). Extra column not created.]\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1986, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1986, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1986, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.076425\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Info] Start training from score -2.080449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1987, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1987, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.076928\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Info] Start training from score -2.080952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Cross-Validation Accuracy Scores: [0.85915493 0.8832998  0.83903421 0.89314516 0.875     ]\n",
      "Mean Cross-Validation Accuracy: 0.8699268189783865\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 2483, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86        77\n",
      "           1       0.91      0.77      0.83        77\n",
      "           2       0.92      0.87      0.89        78\n",
      "           3       0.89      0.97      0.93        78\n",
      "           4       0.88      0.96      0.92        78\n",
      "           5       0.97      0.91      0.94        77\n",
      "           6       0.96      0.97      0.97        78\n",
      "           7       0.85      0.92      0.88        78\n",
      "\n",
      "    accuracy                           0.90       621\n",
      "   macro avg       0.91      0.90      0.90       621\n",
      "weighted avg       0.91      0.90      0.90       621\n",
      "\n",
      "[[66  3  1  0  5  0  2  0]\n",
      " [ 5 59  0  1  3  1  1  7]\n",
      " [ 1  1 68  5  1  0  0  2]\n",
      " [ 0  0  1 76  0  0  0  1]\n",
      " [ 2  1  0  0 75  0  0  0]\n",
      " [ 0  0  1  3  1 70  0  2]\n",
      " [ 1  0  0  0  0  0 76  1]\n",
      " [ 1  1  3  0  0  1  0 72]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "important_features = [\n",
    "    'sFas (pg/ml)', 'sHER2/sEGFR2/sErbB2 (pg/ml)', 'CA 15-3 (U/ml)', 'CA19-9 (U/ml)', 'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)', 'TGFa (pg/ml)', 'Sex_1', 'Leptin (pg/ml)', 'IL-8 (pg/ml)', 'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)', 'GDF15 (ng/ml)', 'Prolactin (pg/ml)', 'HGF (pg/ml)', 'CD44 (ng/ml)', 'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)', 'TIMP-1 (pg/ml)', 'HE4 (pg/ml)'\n",
    "]\n",
    "\n",
    "# Separar características y etiquetas de los datos reales\n",
    "X_real = df[important_features]\n",
    "y_real = df['Tumor type']\n",
    "\n",
    "# Identificar clases minoritarias\n",
    "class_counts = y_real.value_counts()\n",
    "minority_classes = class_counts[class_counts < class_counts.median()].index\n",
    "\n",
    "# Separar datos de clases minoritarias\n",
    "X_minority = X_real[y_real.isin(minority_classes)]\n",
    "y_minority = y_real[y_real.isin(minority_classes)]\n",
    "\n",
    "# Entrenar el modelo CTGAN solo con las clases minoritarias\n",
    "model = CTGAN(epochs=300)\n",
    "model.fit(X_minority)\n",
    "\n",
    "# Generar datos sintéticos para las clases minoritarias\n",
    "synthetic_data_minority = model.sample(len(X_minority))\n",
    "\n",
    "# Asignar etiquetas correctas a los datos sintéticos generados\n",
    "synthetic_data_minority['Tumor type'] = np.random.choice(minority_classes, len(synthetic_data_minority))\n",
    "\n",
    "# Separar características y etiquetas de los datos sintéticos generados\n",
    "X_synthetic = synthetic_data_minority[important_features]\n",
    "y_synthetic = synthetic_data_minority['Tumor type']\n",
    "\n",
    "# Combinar datos reales y datos sintéticos generados\n",
    "X_combined = pd.concat([X_real, X_synthetic], axis=0)\n",
    "y_combined = pd.concat([y_real, y_synthetic], axis=0)\n",
    "\n",
    "# Aplicar SMOTE para sobremuestrear las clases minoritarias en el conjunto combinado\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Definir los modelos individuales con regularización\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "lgbm_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Definir el Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_clf),\n",
    "        ('gb', gb_clf),\n",
    "        ('lgbm', lgbm_clf),\n",
    "        ('xgb', xgb_clf)\n",
    "    ],\n",
    "    voting='soft'  # 'soft' uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Validación cruzada estratificada\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Mean Cross-Validation Accuracy: {cv_scores.mean()}')\n",
    "\n",
    "# Entrenar el Voting Classifier con todos los datos resampleados\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo en el conjunto de prueba\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 13:00:37,661: INFO: 2705523432: Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       149\n",
      "           1       1.00      0.96      0.98       273\n",
      "           2       0.88      0.91      0.89        32\n",
      "           3       0.97      0.97      0.97        33\n",
      "           4       0.96      0.96      0.96        69\n",
      "           5       0.98      1.00      0.99        44\n",
      "           6       0.97      1.00      0.98        57\n",
      "           7       0.90      0.96      0.93        46\n",
      "\n",
      "    accuracy                           0.97       703\n",
      "   macro avg       0.95      0.97      0.96       703\n",
      "weighted avg       0.97      0.97      0.97       703\n",
      "]\n",
      "[2024-06-27 13:00:37,661: INFO: 2705523432: Training Confusion Matrix:\n",
      "[[145   0   1   0   2   0   1   0]\n",
      " [  4 262   2   0   1   1   0   3]\n",
      " [  1   0  29   1   0   0   0   1]\n",
      " [  0   0   0  32   0   0   1   0]\n",
      " [  1   1   0   0  66   0   0   1]\n",
      " [  0   0   0   0   0  44   0   0]\n",
      " [  0   0   0   0   0   0  57   0]\n",
      " [  1   0   1   0   0   0   0  44]]]\n",
      "[2024-06-27 13:00:37,661: INFO: 2705523432: Testing Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        60\n",
      "           1       0.98      0.96      0.97       115\n",
      "           2       0.93      1.00      0.96        13\n",
      "           3       1.00      1.00      1.00        11\n",
      "           4       0.88      1.00      0.93        35\n",
      "           5       1.00      1.00      1.00        10\n",
      "           6       1.00      1.00      1.00        36\n",
      "           7       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.97       302\n",
      "   macro avg       0.97      0.99      0.98       302\n",
      "weighted avg       0.98      0.97      0.97       302\n",
      "]\n",
      "[2024-06-27 13:00:37,661: INFO: 2705523432: Testing Confusion Matrix:\n",
      "[[ 57   2   0   0   1   0   0   0]\n",
      " [  0 110   1   0   4   0   0   0]\n",
      " [  0   0  13   0   0   0   0   0]\n",
      " [  0   0   0  11   0   0   0   0]\n",
      " [  0   0   0   0  35   0   0   0]\n",
      " [  0   0   0   0   0  10   0   0]\n",
      " [  0   0   0   0   0   0  36   0]\n",
      " [  0   0   0   0   0   0   0  22]]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ctgan import CTGAN\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from CancerPrediction.entity.config_entity import ModelTrainerConfig\n",
    "from CancerPrediction import logger\n",
    "import json\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig, seed: int = 42):\n",
    "        self.config = config\n",
    "        self.params = config.params  # Obtener los parámetros desde el config\n",
    "        self.seed = seed\n",
    "        set_seed(self.seed)\n",
    "\n",
    "    def train(self):\n",
    "        # Cargar datos de entrenamiento y prueba\n",
    "        train_data = pd.read_excel(self.config.train_data_path)\n",
    "        test_data = pd.read_excel(self.config.test_data_path)\n",
    "\n",
    "        # Combinar datos de entrenamiento y prueba\n",
    "        df_combined = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "        # Seleccionar características importantes\n",
    "        X_real = df_combined[self.config.important_features]\n",
    "        y_real = df_combined[self.config.target_column]\n",
    "\n",
    "        # Identificar clases minoritarias\n",
    "        class_counts = y_real.value_counts()\n",
    "        minority_classes = class_counts[class_counts < class_counts.median()].index\n",
    "\n",
    "        # Separar datos de clases minoritarias\n",
    "        X_minority = X_real[y_real.isin(minority_classes)]\n",
    "        y_minority = y_real[y_real.isin(minority_classes)]\n",
    "\n",
    "        # Entrenar el modelo CTGAN solo con las clases minoritarias\n",
    "        ctgan_params = self.params['CTGAN']\n",
    "        model = CTGAN(**ctgan_params)\n",
    "        model.fit(X_minority)\n",
    "\n",
    "        # Generar datos sintéticos para las clases minoritarias\n",
    "        synthetic_data_minority = model.sample(len(X_minority))\n",
    "\n",
    "        # Asignar etiquetas correctas a los datos sintéticos generados\n",
    "        synthetic_data_minority[self.config.target_column] = np.random.choice(minority_classes, len(synthetic_data_minority))\n",
    "\n",
    "        # Separar características y etiquetas de los datos sintéticos generados\n",
    "        X_synthetic = synthetic_data_minority[self.config.important_features]\n",
    "        y_synthetic = synthetic_data_minority[self.config.target_column]\n",
    "\n",
    "        # Combinar datos reales y datos sintéticos generados\n",
    "        X_combined = pd.concat([X_real, X_synthetic], axis=0)\n",
    "        y_combined = pd.concat([y_real, y_synthetic], axis=0)\n",
    "\n",
    "        # Aplicar SMOTE para sobremuestrear las clases minoritarias en el conjunto combinado\n",
    "        smote = SMOTE(random_state=self.seed)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)\n",
    "\n",
    "        # Dividir en conjuntos de entrenamiento y prueba\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=self.seed, stratify=y_resampled)\n",
    "\n",
    "        # Definir los modelos individuales con regularización\n",
    "        rf_clf = RandomForestClassifier(random_state=self.seed, **self.params['RandomForest'])\n",
    "        gb_clf = GradientBoostingClassifier(random_state=self.seed, **self.params['GradientBoosting'])\n",
    "        lgbm_clf = lgb.LGBMClassifier(random_state=self.seed, **self.params['LightGBM'])\n",
    "        xgb_clf = xgb.XGBClassifier(random_state=self.seed, **self.params['XGBoost'])\n",
    "\n",
    "        # Definir el Voting Classifier\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_clf),\n",
    "                ('gb', gb_clf),\n",
    "                ('lgbm', lgbm_clf),\n",
    "                ('xgb', xgb_clf)\n",
    "            ],\n",
    "            voting='soft'  # 'soft' uses predicted probabilities\n",
    "        )\n",
    "\n",
    "        # Entrenar el Voting Classifier con todos los datos resampleados\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Guardar el modelo entrenado\n",
    "        joblib.dump(voting_clf, os.path.join(self.config.root_dir, self.config.model_name))\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        logger.info(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n",
    "        logger.info(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-27 13:08:10,288: INFO: null: Guidance: There are no missing values in column sFas (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,327: INFO: null: Guidance: There are no missing values in column sHER2/sEGFR2/sErbB2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,472: INFO: null: Guidance: There are no missing values in column CA 15-3 (U/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,500: INFO: null: Guidance: There are no missing values in column CA19-9 (U/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,522: INFO: null: Guidance: There are no missing values in column CA-125 (U/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,558: INFO: null: Guidance: There are no missing values in column TIMP-2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,708: INFO: null: Guidance: There are no missing values in column TGFa (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,724: INFO: null: Guidance: There are no missing values in column Sex_1. Extra column not created.]\n",
      "[2024-06-27 13:08:10,740: INFO: null: Guidance: There are no missing values in column Leptin (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,791: INFO: null: Guidance: There are no missing values in column IL-8 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,830: INFO: null: Guidance: There are no missing values in column IL-6 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,926: INFO: null: Guidance: There are no missing values in column AFP (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:10,959: INFO: null: Guidance: There are no missing values in column GDF15 (ng/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,036: INFO: null: Guidance: There are no missing values in column Prolactin (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,109: INFO: null: Guidance: There are no missing values in column HGF (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,242: INFO: null: Guidance: There are no missing values in column CD44 (ng/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,373: INFO: null: Guidance: There are no missing values in column Midkine (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,447: INFO: null: Guidance: There are no missing values in column Thrombospondin-2 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,528: INFO: null: Guidance: There are no missing values in column TIMP-1 (pg/ml). Extra column not created.]\n",
      "[2024-06-27 13:08:11,592: INFO: null: Guidance: There are no missing values in column HE4 (pg/ml). Extra column not created.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-2.24) | Discrim. (0.26): 100%|██████████| 300/300 [00:25<00:00, 11.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4847\n",
      "[LightGBM] [Info] Number of data points in the train set: 2483, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.077430\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Info] Start training from score -2.080650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85        77\n",
      "           1       0.93      0.74      0.83        77\n",
      "           2       0.89      0.90      0.89        78\n",
      "           3       0.90      0.90      0.90        78\n",
      "           4       0.79      0.94      0.86        78\n",
      "           5       0.87      0.90      0.88        77\n",
      "           6       0.97      0.96      0.97        78\n",
      "           7       0.81      0.77      0.79        78\n",
      "\n",
      "    accuracy                           0.87       621\n",
      "   macro avg       0.87      0.87      0.87       621\n",
      "weighted avg       0.87      0.87      0.87       621\n",
      "\n",
      "Confusion Matrix:\n",
      " [[67  2  1  0  6  0  1  0]\n",
      " [ 4 57  2  1  7  2  0  4]\n",
      " [ 0  2 70  2  0  1  0  3]\n",
      " [ 0  0  1 70  0  3  1  3]\n",
      " [ 2  0  0  0 73  1  0  2]\n",
      " [ 2  0  1  2  2 69  0  1]\n",
      " [ 1  0  0  0  1  0 75  1]\n",
      " [ 5  0  4  3  3  3  0 60]]\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       149\n",
      "           1       0.99      0.94      0.97       273\n",
      "           2       0.85      0.91      0.88        32\n",
      "           3       0.97      0.94      0.95        33\n",
      "           4       0.88      0.94      0.91        69\n",
      "           5       0.96      1.00      0.98        44\n",
      "           6       0.96      0.96      0.96        57\n",
      "           7       0.89      0.91      0.90        46\n",
      "\n",
      "    accuracy                           0.95       703\n",
      "   macro avg       0.93      0.95      0.94       703\n",
      "weighted avg       0.95      0.95      0.95       703\n",
      "\n",
      "Training Confusion Matrix:\n",
      "[[144   0   1   0   4   0   0   0]\n",
      " [  3 257   3   1   3   1   1   4]\n",
      " [  1   1  29   0   1   0   0   0]\n",
      " [  0   1   0  31   0   0   1   0]\n",
      " [  2   0   0   0  65   1   0   1]\n",
      " [  0   0   0   0   0  44   0   0]\n",
      " [  2   0   0   0   0   0  55   0]\n",
      " [  2   0   1   0   1   0   0  42]]\n",
      "Testing Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        60\n",
      "           1       0.98      0.92      0.95       115\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       0.92      1.00      0.96        11\n",
      "           4       0.83      1.00      0.91        35\n",
      "           5       0.91      1.00      0.95        10\n",
      "           6       1.00      0.97      0.99        36\n",
      "           7       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.94       302\n",
      "   macro avg       0.93      0.95      0.94       302\n",
      "weighted avg       0.95      0.94      0.94       302\n",
      "\n",
      "Testing Confusion Matrix:\n",
      "[[ 57   2   0   0   1   0   0   0]\n",
      " [  2 106   0   0   5   1   0   1]\n",
      " [  1   0  12   0   0   0   0   0]\n",
      " [  0   0   0  11   0   0   0   0]\n",
      " [  0   0   0   0  35   0   0   0]\n",
      " [  0   0   0   0   0  10   0   0]\n",
      " [  0   0   0   0   0   0  35   1]\n",
      " [  1   0   0   1   1   0   0  19]]\n"
     ]
    }
   ],
   "source": [
    "# Definir los parámetros ajustados para evitar el sobreajuste\n",
    "params = {\n",
    "    'CTGAN': {\n",
    "        'epochs': 300,\n",
    "        'batch_size': 500,\n",
    "        'discriminator_steps': 1,\n",
    "        'log_frequency': True,\n",
    "        'verbose': True\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 5  # Reducir la profundidad máxima para evitar sobreajuste\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.05,  # Reducir la tasa de aprendizaje para hacer el modelo más robusto\n",
    "        'max_depth': 3\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
    "        'num_leaves': 31\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
    "        'max_depth': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ctgan import CTGAN\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, train_data_path, test_data_path, important_features, target_column, params, model_save_path, seed: int = 42):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.important_features = important_features\n",
    "        self.target_column = target_column\n",
    "        self.params = params\n",
    "        self.model_save_path = model_save_path\n",
    "        self.seed = seed\n",
    "        set_seed(self.seed)\n",
    "\n",
    "    def train(self):\n",
    "        # Cargar datos de entrenamiento y prueba\n",
    "        train_data = pd.read_excel(self.train_data_path)\n",
    "        test_data = pd.read_excel(self.test_data_path)\n",
    "\n",
    "        # Combinar datos de entrenamiento y prueba\n",
    "        df_combined = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "        # Seleccionar características importantes\n",
    "        X_real = df_combined[self.important_features]\n",
    "        y_real = df_combined[self.target_column]\n",
    "\n",
    "        # Identificar clases minoritarias\n",
    "        class_counts = y_real.value_counts()\n",
    "        minority_classes = class_counts[class_counts < class_counts.median()].index\n",
    "\n",
    "        # Separar datos de clases minoritarias\n",
    "        X_minority = X_real[y_real.isin(minority_classes)]\n",
    "        y_minority = y_real[y_real.isin(minority_classes)]\n",
    "\n",
    "        # Entrenar el modelo CTGAN solo con las clases minoritarias\n",
    "        ctgan_params = self.params['CTGAN']\n",
    "        model = CTGAN(**ctgan_params)\n",
    "        model.fit(X_minority)\n",
    "\n",
    "        # Generar datos sintéticos para las clases minoritarias\n",
    "        synthetic_data_minority = model.sample(len(X_minority))\n",
    "\n",
    "        # Asignar etiquetas correctas a los datos sintéticos generados\n",
    "        synthetic_data_minority[self.target_column] = np.random.choice(minority_classes, len(synthetic_data_minority))\n",
    "\n",
    "        # Separar características y etiquetas de los datos sintéticos generados\n",
    "        X_synthetic = synthetic_data_minority[self.important_features]\n",
    "        y_synthetic = synthetic_data_minority[self.target_column]\n",
    "\n",
    "        # Combinar datos reales y datos sintéticos generados\n",
    "        X_combined = pd.concat([X_real, X_synthetic], axis=0)\n",
    "        y_combined = pd.concat([y_real, y_synthetic], axis=0)\n",
    "\n",
    "        # Aplicar SMOTE para sobremuestrear las clases minoritarias en el conjunto combinado\n",
    "        smote = SMOTE(random_state=self.seed)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)\n",
    "\n",
    "        # Dividir en conjuntos de entrenamiento y prueba\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=self.seed, stratify=y_resampled)\n",
    "\n",
    "        # Definir los modelos individuales con regularización\n",
    "        rf_clf = RandomForestClassifier(random_state=self.seed, **self.params['RandomForest'])\n",
    "        gb_clf = GradientBoostingClassifier(random_state=self.seed, **self.params['GradientBoosting'])\n",
    "        lgbm_clf = lgb.LGBMClassifier(random_state=self.seed, **self.params['LightGBM'])\n",
    "        xgb_clf = xgb.XGBClassifier(random_state=self.seed, **self.params['XGBoost'])\n",
    "\n",
    "        # Definir el Voting Classifier\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_clf),\n",
    "                ('gb', gb_clf),\n",
    "                ('lgbm', lgbm_clf),\n",
    "                ('xgb', xgb_clf)\n",
    "            ],\n",
    "            voting='soft'  # 'soft' uses predicted probabilities\n",
    "        )\n",
    "\n",
    "        # Entrenar el Voting Classifier con todos los datos resampleados\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Guardar el modelo entrenado\n",
    "        joblib.dump(voting_clf, self.model_save_path)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        print(f\"Classification Report:\\n {classification_report(y_test, y_pred)}\")\n",
    "        print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "# Definir las rutas de los datos y los parámetros\n",
    "train_data_path = \"artifacts/data_transformation/train_df.xlsx\"\n",
    "test_data_path = \"artifacts/data_transformation/test_df.xlsx\"\n",
    "important_features = [\n",
    "    'sFas (pg/ml)', 'sHER2/sEGFR2/sErbB2 (pg/ml)', 'CA 15-3 (U/ml)', 'CA19-9 (U/ml)', 'CA-125 (U/ml)',\n",
    "    'TIMP-2 (pg/ml)', 'TGFa (pg/ml)', 'Sex_1', 'Leptin (pg/ml)', 'IL-8 (pg/ml)', 'IL-6 (pg/ml)',\n",
    "    'AFP (pg/ml)', 'GDF15 (ng/ml)', 'Prolactin (pg/ml)', 'HGF (pg/ml)', 'CD44 (ng/ml)', 'Midkine (pg/ml)',\n",
    "    'Thrombospondin-2 (pg/ml)', 'TIMP-1 (pg/ml)', 'HE4 (pg/ml)'\n",
    "]\n",
    "target_column = \"Tumor type\"\n",
    "model_save_path = \"artifacts/model_trainer/model.joblib\"\n",
    "\n",
    "# Entrenar el modelo con los nuevos parámetros\n",
    "model_trainer = ModelTrainer(train_data_path, test_data_path, important_features, target_column, params, model_save_path)\n",
    "model_trainer.train()\n",
    "\n",
    "\n",
    "# Evaluar el sobreajuste\n",
    "train_data = pd.read_excel(train_data_path)\n",
    "test_data = pd.read_excel(test_data_path)\n",
    "X_train = train_data[important_features]\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data[important_features]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "model = joblib.load(model_save_path)\n",
    "\n",
    "# Predicciones en conjunto de entrenamiento\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_report = classification_report(y_train, y_train_pred)\n",
    "train_conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Predicciones en conjunto de prueba\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Comparar métricas\n",
    "print(f\"Training Classification Report:\\n{train_report}\")\n",
    "print(f\"Training Confusion Matrix:\\n{train_conf_matrix}\")\n",
    "print(f\"Testing Classification Report:\\n{test_report}\")\n",
    "print(f\"Testing Confusion Matrix:\\n{test_conf_matrix}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
